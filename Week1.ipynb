{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "from nltk.corpus import brown"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<bound method CategorizedTaggedCorpusReader.words of <CategorizedTaggedCorpusReader in 'C:\\\\Users\\\\Hrithik Jha\\\\AppData\\\\Roaming\\\\nltk_data\\\\corpora\\\\brown'>>"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "brown.words"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<bound method CategorizedTaggedCorpusReader.words of <CategorizedTaggedCorpusReader in 'C:\\\\Users\\\\Hrithik Jha\\\\AppData\\\\Roaming\\\\nltk_data\\\\corpora\\\\brown'>>\n"
     ]
    }
   ],
   "source": [
    "print(brown.words)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "showing info https://raw.githubusercontent.com/nltk/nltk_data/gh-pages/index.xml\n"
     ]
    }
   ],
   "source": [
    "import nltk\n",
    "nltk.download()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "nltk.download()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "4\n"
     ]
    }
   ],
   "source": [
    "print(2+2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "'Hello'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "hello\n"
     ]
    }
   ],
   "source": [
    "print(\"hello\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'brown' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-3-1cb0bf88051c>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m()\u001b[0m\n\u001b[1;32m----> 1\u001b[1;33m \u001b[0mprint\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mbrown\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mwords\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m: name 'brown' is not defined"
     ]
    }
   ],
   "source": [
    "print(brown.words)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "from nltk.corpus import brown"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<bound method CategorizedTaggedCorpusReader.words of <CategorizedTaggedCorpusReader in 'C:\\\\Users\\\\Hrithik Jha\\\\AppData\\\\Roaming\\\\nltk_data\\\\corpora\\\\brown'>>\n"
     ]
    }
   ],
   "source": [
    "print(brown.words)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "import nltk"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "*** Introductory Examples for the NLTK Book ***\n",
      "Loading text1, ..., text9 and sent1, ..., sent9\n",
      "Type the name of the text or sentence to view it.\n",
      "Type: 'texts()' or 'sents()' to list the materials.\n",
      "text1: Moby Dick by Herman Melville 1851\n",
      "text2: Sense and Sensibility by Jane Austen 1811\n",
      "text3: The Book of Genesis\n",
      "text4: Inaugural Address Corpus\n",
      "text5: Chat Corpus\n"
     ]
    },
    {
     "ename": "LookupError",
     "evalue": "\n**********************************************************************\n  Resource \u001b[93mwebtext\u001b[0m not found.\n  Please use the NLTK Downloader to obtain the resource:\n\n  \u001b[31m>>> import nltk\n  >>> nltk.download('webtext')\n  \u001b[0m\n  Searched in:\n    - 'C:\\\\Users\\\\Hrithik Jha\\\\AppData\\\\Roaming\\\\SPB_Data/nltk_data'\n    - 'C:\\\\nltk_data'\n    - 'D:\\\\nltk_data'\n    - 'E:\\\\nltk_data'\n    - 'C:\\\\Anaconda3\\\\nltk_data'\n    - 'C:\\\\Anaconda3\\\\lib\\\\nltk_data'\n    - 'C:\\\\Users\\\\Hrithik Jha\\\\AppData\\\\Roaming\\\\nltk_data'\n**********************************************************************\n",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mLookupError\u001b[0m                               Traceback (most recent call last)",
      "\u001b[1;32mC:\\Anaconda3\\lib\\site-packages\\nltk\\corpus\\util.py\u001b[0m in \u001b[0;36m__load\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m     79\u001b[0m             \u001b[1;32mexcept\u001b[0m \u001b[0mLookupError\u001b[0m \u001b[1;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 80\u001b[1;33m                 \u001b[1;32mtry\u001b[0m\u001b[1;33m:\u001b[0m \u001b[0mroot\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mnltk\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mdata\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mfind\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m'{}/{}'\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mformat\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0msubdir\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mzip_name\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     81\u001b[0m                 \u001b[1;32mexcept\u001b[0m \u001b[0mLookupError\u001b[0m\u001b[1;33m:\u001b[0m \u001b[1;32mraise\u001b[0m \u001b[0me\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mC:\\Anaconda3\\lib\\site-packages\\nltk\\data.py\u001b[0m in \u001b[0;36mfind\u001b[1;34m(resource_name, paths)\u001b[0m\n\u001b[0;32m    672\u001b[0m     \u001b[0mresource_not_found\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;34m'\\n%s\\n%s\\n%s\\n'\u001b[0m \u001b[1;33m%\u001b[0m \u001b[1;33m(\u001b[0m\u001b[0msep\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mmsg\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0msep\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 673\u001b[1;33m     \u001b[1;32mraise\u001b[0m \u001b[0mLookupError\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mresource_not_found\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    674\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mLookupError\u001b[0m: \n**********************************************************************\n  Resource \u001b[93mwebtext\u001b[0m not found.\n  Please use the NLTK Downloader to obtain the resource:\n\n  \u001b[31m>>> import nltk\n  >>> nltk.download('webtext')\n  \u001b[0m\n  Searched in:\n    - 'C:\\\\Users\\\\Hrithik Jha\\\\AppData\\\\Roaming\\\\SPB_Data/nltk_data'\n    - 'C:\\\\nltk_data'\n    - 'D:\\\\nltk_data'\n    - 'E:\\\\nltk_data'\n    - 'C:\\\\Anaconda3\\\\nltk_data'\n    - 'C:\\\\Anaconda3\\\\lib\\\\nltk_data'\n    - 'C:\\\\Users\\\\Hrithik Jha\\\\AppData\\\\Roaming\\\\nltk_data'\n**********************************************************************\n",
      "\nDuring handling of the above exception, another exception occurred:\n",
      "\u001b[1;31mLookupError\u001b[0m                               Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-10-a6ea6dd70c17>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m()\u001b[0m\n\u001b[1;32m----> 1\u001b[1;33m \u001b[1;32mfrom\u001b[0m \u001b[0mnltk\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mbook\u001b[0m \u001b[1;32mimport\u001b[0m \u001b[1;33m*\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[1;32mC:\\Anaconda3\\lib\\site-packages\\nltk\\book.py\u001b[0m in \u001b[0;36m<module>\u001b[1;34m()\u001b[0m\n\u001b[0;32m     34\u001b[0m \u001b[0mprint\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m\"text5:\"\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mtext5\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mname\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     35\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 36\u001b[1;33m text6 = Text(webtext.words('grail.txt'),\n\u001b[0m\u001b[0;32m     37\u001b[0m              name=\"Monty Python and the Holy Grail\")\n\u001b[0;32m     38\u001b[0m \u001b[0mprint\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m\"text6:\"\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mtext6\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mname\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mC:\\Anaconda3\\lib\\site-packages\\nltk\\corpus\\util.py\u001b[0m in \u001b[0;36m__getattr__\u001b[1;34m(self, attr)\u001b[0m\n\u001b[0;32m    114\u001b[0m             \u001b[1;32mraise\u001b[0m \u001b[0mAttributeError\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m\"LazyCorpusLoader object has no attribute '__bases__'\"\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    115\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 116\u001b[1;33m         \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m__load\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    117\u001b[0m         \u001b[1;31m# This looks circular, but its not, since __load() changes our\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    118\u001b[0m         \u001b[1;31m# __class__ to something new:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mC:\\Anaconda3\\lib\\site-packages\\nltk\\corpus\\util.py\u001b[0m in \u001b[0;36m__load\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m     79\u001b[0m             \u001b[1;32mexcept\u001b[0m \u001b[0mLookupError\u001b[0m \u001b[1;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     80\u001b[0m                 \u001b[1;32mtry\u001b[0m\u001b[1;33m:\u001b[0m \u001b[0mroot\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mnltk\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mdata\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mfind\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m'{}/{}'\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mformat\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0msubdir\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mzip_name\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 81\u001b[1;33m                 \u001b[1;32mexcept\u001b[0m \u001b[0mLookupError\u001b[0m\u001b[1;33m:\u001b[0m \u001b[1;32mraise\u001b[0m \u001b[0me\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     82\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     83\u001b[0m         \u001b[1;31m# Load the corpus.\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mC:\\Anaconda3\\lib\\site-packages\\nltk\\corpus\\util.py\u001b[0m in \u001b[0;36m__load\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m     76\u001b[0m         \u001b[1;32melse\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     77\u001b[0m             \u001b[1;32mtry\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 78\u001b[1;33m                 \u001b[0mroot\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mnltk\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mdata\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mfind\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m'{}/{}'\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mformat\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0msubdir\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m__name\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     79\u001b[0m             \u001b[1;32mexcept\u001b[0m \u001b[0mLookupError\u001b[0m \u001b[1;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     80\u001b[0m                 \u001b[1;32mtry\u001b[0m\u001b[1;33m:\u001b[0m \u001b[0mroot\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mnltk\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mdata\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mfind\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m'{}/{}'\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mformat\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0msubdir\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mzip_name\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mC:\\Anaconda3\\lib\\site-packages\\nltk\\data.py\u001b[0m in \u001b[0;36mfind\u001b[1;34m(resource_name, paths)\u001b[0m\n\u001b[0;32m    671\u001b[0m     \u001b[0msep\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;34m'*'\u001b[0m \u001b[1;33m*\u001b[0m \u001b[1;36m70\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    672\u001b[0m     \u001b[0mresource_not_found\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;34m'\\n%s\\n%s\\n%s\\n'\u001b[0m \u001b[1;33m%\u001b[0m \u001b[1;33m(\u001b[0m\u001b[0msep\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mmsg\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0msep\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 673\u001b[1;33m     \u001b[1;32mraise\u001b[0m \u001b[0mLookupError\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mresource_not_found\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    674\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    675\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mLookupError\u001b[0m: \n**********************************************************************\n  Resource \u001b[93mwebtext\u001b[0m not found.\n  Please use the NLTK Downloader to obtain the resource:\n\n  \u001b[31m>>> import nltk\n  >>> nltk.download('webtext')\n  \u001b[0m\n  Searched in:\n    - 'C:\\\\Users\\\\Hrithik Jha\\\\AppData\\\\Roaming\\\\SPB_Data/nltk_data'\n    - 'C:\\\\nltk_data'\n    - 'D:\\\\nltk_data'\n    - 'E:\\\\nltk_data'\n    - 'C:\\\\Anaconda3\\\\nltk_data'\n    - 'C:\\\\Anaconda3\\\\lib\\\\nltk_data'\n    - 'C:\\\\Users\\\\Hrithik Jha\\\\AppData\\\\Roaming\\\\nltk_data'\n**********************************************************************\n"
     ]
    }
   ],
   "source": [
    "from nltk.book import *"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "from nltk.corpus import brown"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<bound method CategorizedTaggedCorpusReader.words of <CategorizedTaggedCorpusReader in 'C:\\\\Users\\\\Hrithik Jha\\\\AppData\\\\Roaming\\\\nltk_data\\\\corpora\\\\brown'>>"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "brown.words"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Hallelujah\n"
     ]
    }
   ],
   "source": [
    "print(\"Hallelujah\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<bound method CategorizedTaggedCorpusReader.words of <CategorizedTaggedCorpusReader in 'C:\\\\Users\\\\Hrithik Jha\\\\AppData\\\\Roaming\\\\nltk_data\\\\corpora\\\\brown'>>\n"
     ]
    }
   ],
   "source": [
    "print(brown.words)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import nltk\n",
    "from nltk.corpus import inaugural"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['1789-Washington.txt',\n",
       " '1793-Washington.txt',\n",
       " '1797-Adams.txt',\n",
       " '1801-Jefferson.txt',\n",
       " '1805-Jefferson.txt',\n",
       " '1809-Madison.txt',\n",
       " '1813-Madison.txt',\n",
       " '1817-Monroe.txt',\n",
       " '1821-Monroe.txt',\n",
       " '1825-Adams.txt',\n",
       " '1829-Jackson.txt',\n",
       " '1833-Jackson.txt',\n",
       " '1837-VanBuren.txt',\n",
       " '1841-Harrison.txt',\n",
       " '1845-Polk.txt',\n",
       " '1849-Taylor.txt',\n",
       " '1853-Pierce.txt',\n",
       " '1857-Buchanan.txt',\n",
       " '1861-Lincoln.txt',\n",
       " '1865-Lincoln.txt',\n",
       " '1869-Grant.txt',\n",
       " '1873-Grant.txt',\n",
       " '1877-Hayes.txt',\n",
       " '1881-Garfield.txt',\n",
       " '1885-Cleveland.txt',\n",
       " '1889-Harrison.txt',\n",
       " '1893-Cleveland.txt',\n",
       " '1897-McKinley.txt',\n",
       " '1901-McKinley.txt',\n",
       " '1905-Roosevelt.txt',\n",
       " '1909-Taft.txt',\n",
       " '1913-Wilson.txt',\n",
       " '1917-Wilson.txt',\n",
       " '1921-Harding.txt',\n",
       " '1925-Coolidge.txt',\n",
       " '1929-Hoover.txt',\n",
       " '1933-Roosevelt.txt',\n",
       " '1937-Roosevelt.txt',\n",
       " '1941-Roosevelt.txt',\n",
       " '1945-Roosevelt.txt',\n",
       " '1949-Truman.txt',\n",
       " '1953-Eisenhower.txt',\n",
       " '1957-Eisenhower.txt',\n",
       " '1961-Kennedy.txt',\n",
       " '1965-Johnson.txt',\n",
       " '1969-Nixon.txt',\n",
       " '1973-Nixon.txt',\n",
       " '1977-Carter.txt',\n",
       " '1981-Reagan.txt',\n",
       " '1985-Reagan.txt',\n",
       " '1989-Bush.txt',\n",
       " '1993-Clinton.txt',\n",
       " '1997-Clinton.txt',\n",
       " '2001-Bush.txt',\n",
       " '2005-Bush.txt',\n",
       " '2009-Obama.txt']"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "inaugural.fileids()"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "texts = inaugural.words(\"2009-Obama.txt\")[:100]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "texts = inaugural.words(\"2009-Obama.txt\")[:100]\n",
    "for text in texts:\n",
    "    sentences = nltk.sent_tokenize(text)\n",
    "    for sentence in sentences:\n",
    "        word = word_tokenize(sentence)\n",
    "        tagged = nltk.pos_tag(word)\n",
    "        print(tagged)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['My', 'fellow', 'citizens', ':', 'I', 'stand', 'here', 'today', 'humbled', 'by', 'the', 'task', 'before', 'us', ',', 'grateful', 'for', 'the', 'trust', 'you', 'have', 'bestowed', ',', 'mindful', 'of', 'the', 'sacrifices', 'borne', 'by', 'our', 'ancestors', '.', 'I', 'thank', 'President', 'Bush', 'for', 'his', 'service', 'to', 'our', 'nation', ',', 'as', 'well', 'as', 'the', 'generosity', 'and', 'cooperation', 'he', 'has', 'shown', 'throughout', 'this', 'transition', '.', 'Forty', '-', 'four', 'Americans', 'have', 'now', 'taken', 'the', 'presidential', 'oath', '.', 'The', 'words', 'have', 'been', 'spoken', 'during', 'rising', 'tides', 'of', 'prosperity', 'and', 'the', 'still', 'waters', 'of', 'peace', '.', 'Yet', ',', 'every', 'so', 'often', 'the', 'oath', 'is', 'taken', 'amidst', 'gathering', 'clouds', 'and', 'raging', 'storms']\n"
     ]
    }
   ],
   "source": [
    "text = inaugural.words(\"2009-Obama.txt\")[:100]\n",
    "print(text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[('M', 'NNP'), ('y', 'NN')]\n",
      "[('f', 'JJ'), ('e', 'NN'), ('l', 'NN'), ('l', 'NN'), ('o', 'NN'), ('w', 'NN')]\n",
      "[('c', 'NN'), ('i', 'NN'), ('t', 'VBP'), ('i', 'JJ'), ('z', 'VBP'), ('e', 'JJ'), ('n', 'NNS'), ('s', 'VBP')]\n",
      "[(':', ':')]\n",
      "[('I', 'PRP')]\n",
      "[('s', 'NN'), ('t', 'VBZ'), ('a', 'DT'), ('n', 'JJ'), ('d', 'NN')]\n",
      "[('h', 'NN'), ('e', 'NN'), ('r', 'NN'), ('e', 'NN')]\n",
      "[('t', 'NN'), ('o', 'MD'), ('d', 'VB'), ('a', 'DT'), ('y', 'NN')]\n",
      "[('h', 'NN'), ('u', 'JJ'), ('m', 'NN'), ('b', 'NN'), ('l', 'NN'), ('e', 'NN'), ('d', 'NN')]\n",
      "[('b', 'NN'), ('y', 'NN')]\n",
      "[('t', 'NN'), ('h', 'NN'), ('e', 'NN')]\n",
      "[('t', 'VB'), ('a', 'DT'), ('s', 'NN'), ('k', 'NN')]\n",
      "[('b', 'NN'), ('e', 'NN'), ('f', 'NN'), ('o', 'NN'), ('r', 'NN'), ('e', 'NN')]\n",
      "[('u', 'JJ'), ('s', 'NN')]\n",
      "[(',', ',')]\n",
      "[('g', 'NN'), ('r', 'VBZ'), ('a', 'DT'), ('t', 'NN'), ('e', 'NN'), ('f', 'NN'), ('u', 'JJ'), ('l', 'NN')]\n",
      "[('f', 'NN'), ('o', 'NN'), ('r', 'NN')]\n",
      "[('t', 'NN'), ('h', 'NN'), ('e', 'NN')]\n",
      "[('t', 'NN'), ('r', 'NN'), ('u', 'JJ'), ('s', 'NN'), ('t', 'NN')]\n",
      "[('y', 'NN'), ('o', 'NN'), ('u', 'NN')]\n",
      "[('h', 'VB'), ('a', 'DT'), ('v', 'NN'), ('e', 'NN')]\n",
      "[('b', 'NN'), ('e', 'NN'), ('s', 'NN'), ('t', 'NN'), ('o', 'NN'), ('w', 'NN'), ('e', 'NN'), ('d', 'NN')]\n",
      "[(',', ',')]\n",
      "[('m', 'NN'), ('i', 'NN'), ('n', 'VBP'), ('d', 'NN'), ('f', 'NN'), ('u', 'JJ'), ('l', 'NN')]\n",
      "[('o', 'NN'), ('f', 'NN')]\n",
      "[('t', 'NN'), ('h', 'NN'), ('e', 'NN')]\n",
      "[('s', 'VB'), ('a', 'DT'), ('c', 'JJ'), ('r', 'NN'), ('i', 'NN'), ('f', 'VBP'), ('i', 'JJ'), ('c', 'VBP'), ('e', 'NN'), ('s', 'NN')]\n",
      "[('b', 'NN'), ('o', 'NN'), ('r', 'NN'), ('n', 'NN'), ('e', 'NN')]\n",
      "[('b', 'NN'), ('y', 'NN')]\n",
      "[('o', 'NN'), ('u', 'JJ'), ('r', 'NN')]\n",
      "[('a', 'DT'), ('n', 'JJ'), ('c', 'NN'), ('e', 'NN'), ('s', 'JJ'), ('t', 'NN'), ('o', 'NN'), ('r', 'NN'), ('s', 'NN')]\n",
      "[('.', '.')]\n",
      "[('I', 'PRP')]\n",
      "[('t', 'NN'), ('h', 'VBZ'), ('a', 'DT'), ('n', 'JJ'), ('k', 'NN')]\n",
      "[('P', 'NNP'), ('r', 'NN'), ('e', 'NN'), ('s', 'NN'), ('i', 'NN'), ('d', 'VBP'), ('e', 'NN'), ('n', 'NNS'), ('t', 'VBP')]\n",
      "[('B', 'NNP'), ('u', 'JJ'), ('s', 'NN'), ('h', 'NN')]\n",
      "[('f', 'NN'), ('o', 'NN'), ('r', 'NN')]\n",
      "[('h', 'NN'), ('i', 'NN'), ('s', 'VBP')]\n",
      "[('s', 'NN'), ('e', 'NN'), ('r', 'NN'), ('v', 'NN'), ('i', 'NN'), ('c', 'VBP'), ('e', 'NN')]\n",
      "[('t', 'NN'), ('o', 'NN')]\n",
      "[('o', 'NN'), ('u', 'JJ'), ('r', 'NN')]\n",
      "[('n', 'RB'), ('a', 'DT'), ('t', 'NN'), ('i', 'NN'), ('o', 'VBP'), ('n', 'NN')]\n",
      "[(',', ',')]\n",
      "[('a', 'DT'), ('s', 'NN')]\n",
      "[('w', 'NN'), ('e', 'NN'), ('l', 'NN'), ('l', 'NN')]\n",
      "[('a', 'DT'), ('s', 'NN')]\n",
      "[('t', 'NN'), ('h', 'NN'), ('e', 'NN')]\n",
      "[('g', 'NN'), ('e', 'NN'), ('n', 'JJ'), ('e', 'NN'), ('r', 'NN'), ('o', 'IN'), ('s', 'NN'), ('i', 'NN'), ('t', 'VBP'), ('y', 'NN')]\n",
      "[('a', 'DT'), ('n', 'JJ'), ('d', 'NN')]\n",
      "[('c', 'NNS'), ('o', 'VBP'), ('o', 'JJ'), ('p', 'NN'), ('e', 'NN'), ('r', 'VBZ'), ('a', 'DT'), ('t', 'NN'), ('i', 'NN'), ('o', 'VBP'), ('n', 'NN')]\n",
      "[('h', 'NN'), ('e', 'NN')]\n",
      "[('h', 'VB'), ('a', 'DT'), ('s', 'NN')]\n",
      "[('s', 'NN'), ('h', 'NN'), ('o', 'JJ'), ('w', 'NN'), ('n', 'NN')]\n",
      "[('t', 'NN'), ('h', 'NN'), ('r', 'NN'), ('o', 'NN'), ('u', 'JJ'), ('g', 'NN'), ('h', 'NN'), ('o', 'JJ'), ('u', 'JJ'), ('t', 'NN')]\n",
      "[('t', 'NN'), ('h', 'NN'), ('i', 'NN'), ('s', 'VBP')]\n",
      "[('t', 'NN'), ('r', 'VBZ'), ('a', 'DT'), ('n', 'JJ'), ('s', 'NN'), ('i', 'NN'), ('t', 'VBP'), ('i', 'NN'), ('o', 'VBP'), ('n', 'NN')]\n",
      "[('.', '.')]\n",
      "[('F', 'NNP'), ('o', 'MD'), ('r', 'VB'), ('t', 'NN'), ('y', 'NN')]\n",
      "[('-', ':')]\n",
      "[('f', 'JJ'), ('o', 'NN'), ('u', 'JJ'), ('r', 'NN')]\n",
      "[('A', 'DT'), ('m', 'NN'), ('e', 'NN'), ('r', 'NN'), ('i', 'NN'), ('c', 'VBP'), ('a', 'DT'), ('n', 'JJ'), ('s', 'NN')]\n",
      "[('h', 'VB'), ('a', 'DT'), ('v', 'NN'), ('e', 'NN')]\n",
      "[('n', 'JJ'), ('o', 'NN'), ('w', 'NN')]\n",
      "[('t', 'VB'), ('a', 'DT'), ('k', 'NN'), ('e', 'NN'), ('n', 'NN')]\n",
      "[('t', 'NN'), ('h', 'NN'), ('e', 'NN')]\n",
      "[('p', 'NN'), ('r', 'NN'), ('e', 'NN'), ('s', 'NN'), ('i', 'NN'), ('d', 'VBP'), ('e', 'NN'), ('n', 'JJ'), ('t', 'NN'), ('i', 'VBZ'), ('a', 'DT'), ('l', 'NN')]\n",
      "[('o', 'IN'), ('a', 'DT'), ('t', 'NN'), ('h', 'NN')]\n",
      "[('.', '.')]\n",
      "[('T', 'NNP'), ('h', 'NN'), ('e', 'NN')]\n",
      "[('w', 'NN'), ('o', 'MD'), ('r', 'VB'), ('d', 'NN'), ('s', 'NN')]\n",
      "[('h', 'VB'), ('a', 'DT'), ('v', 'NN'), ('e', 'NN')]\n",
      "[('b', 'NN'), ('e', 'NN'), ('e', 'NN'), ('n', 'NN')]\n",
      "[('s', 'NN'), ('p', 'NN'), ('o', 'NN'), ('k', 'NN'), ('e', 'NN'), ('n', 'NN')]\n",
      "[('d', 'NN'), ('u', 'JJ'), ('r', 'NN'), ('i', 'NN'), ('n', 'VBP'), ('g', 'NN')]\n",
      "[('r', 'NN'), ('i', 'NN'), ('s', 'VBP'), ('i', 'NN'), ('n', 'VBP'), ('g', 'NN')]\n",
      "[('t', 'NN'), ('i', 'NN'), ('d', 'VBP'), ('e', 'NN'), ('s', 'NN')]\n",
      "[('o', 'NN'), ('f', 'NN')]\n",
      "[('p', 'NN'), ('r', 'NN'), ('o', 'NN'), ('s', 'NN'), ('p', 'NN'), ('e', 'NN'), ('r', 'NN'), ('i', 'NN'), ('t', 'VBP'), ('y', 'NN')]\n",
      "[('a', 'DT'), ('n', 'JJ'), ('d', 'NN')]\n",
      "[('t', 'NN'), ('h', 'NN'), ('e', 'NN')]\n",
      "[('s', 'JJ'), ('t', 'NN'), ('i', 'NN'), ('l', 'VBP'), ('l', 'NN')]\n",
      "[('w', 'VB'), ('a', 'DT'), ('t', 'NN'), ('e', 'NN'), ('r', 'NN'), ('s', 'NN')]\n",
      "[('o', 'NN'), ('f', 'NN')]\n",
      "[('p', 'NN'), ('e', 'VBZ'), ('a', 'DT'), ('c', 'NN'), ('e', 'NN')]\n",
      "[('.', '.')]\n",
      "[('Y', 'NNP'), ('e', 'NN'), ('t', 'NN')]\n",
      "[(',', ',')]\n",
      "[('e', 'NN'), ('v', 'NN'), ('e', 'NN'), ('r', 'NN'), ('y', 'NN')]\n",
      "[('s', 'NN'), ('o', 'NN')]\n",
      "[('o', 'JJ'), ('f', 'NN'), ('t', 'NN'), ('e', 'NN'), ('n', 'NN')]\n",
      "[('t', 'NN'), ('h', 'NN'), ('e', 'NN')]\n",
      "[('o', 'IN'), ('a', 'DT'), ('t', 'NN'), ('h', 'NN')]\n",
      "[('i', 'NN'), ('s', 'VBP')]\n",
      "[('t', 'VB'), ('a', 'DT'), ('k', 'NN'), ('e', 'NN'), ('n', 'NN')]\n",
      "[('a', 'DT'), ('m', 'NN'), ('i', 'NN'), ('d', 'VBP'), ('s', 'NN'), ('t', 'NN')]\n",
      "[('g', 'VB'), ('a', 'DT'), ('t', 'NN'), ('h', 'NN'), ('e', 'NN'), ('r', 'NN'), ('i', 'NN'), ('n', 'VBP'), ('g', 'NN')]\n",
      "[('c', 'NNS'), ('l', 'VBP'), ('o', 'JJ'), ('u', 'JJ'), ('d', 'NN'), ('s', 'NN')]\n",
      "[('a', 'DT'), ('n', 'JJ'), ('d', 'NN')]\n",
      "[('r', 'VB'), ('a', 'DT'), ('g', 'NN'), ('i', 'NN'), ('n', 'VBP'), ('g', 'NN')]\n",
      "[('s', 'NN'), ('t', 'NN'), ('o', 'NN'), ('r', 'NN'), ('m', 'NN'), ('s', 'NN')]\n"
     ]
    }
   ],
   "source": [
    "for texts in text:\n",
    "    tagged = nltk.pos_tag(texts)\n",
    "    print(tagged)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package averaged_perceptron_tagger to\n",
      "[nltk_data]     C:\\Users\\Hrithik Jha\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Unzipping taggers\\averaged_perceptron_tagger.zip.\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import nltk\n",
    "nltk.download('averaged_perceptron_tagger')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "from nltk.corpus import brown\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['The', 'Fulton', 'County', 'Grand', 'Jury', 'said', ...]"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "brown.words()[:100]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['adventure',\n",
       " 'belles_lettres',\n",
       " 'editorial',\n",
       " 'fiction',\n",
       " 'government',\n",
       " 'hobbies',\n",
       " 'humor',\n",
       " 'learned',\n",
       " 'lore',\n",
       " 'mystery',\n",
       " 'news',\n",
       " 'religion',\n",
       " 'reviews',\n",
       " 'romance',\n",
       " 'science_fiction']"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "brown.categories()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['Dan', 'Morgan', 'told', 'himself', 'he', 'would', ...]"
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "brown.words(categories = \"adventure\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "texts = [\"Gatwick airport, the UK’s second largest airport, was closed for most of Wednesday night and Thursday morning after it received reports of two drones flying nearby, reports The Guardian. The airport initially had its flights suspended at 9PM on Wednesday evening after the drones were spotted, and although it briefly reopened at 3am, it was forced to close once more 45 minutes later after the drone flights resumed. As of 9:15AM, Thursday morning, flights to and from the airport remained suspended. As well as preventing any flights from taking off, the suspension means that numerous inbound flights had to be diverted to other London-area airports including Luton, Heathrow and Stansted, while other flights were forced to land in Paris and Amsterdam. In total, 760 flights containing 110,000 passengers were due to either take off or land from Gatwick over the course of Thursday.\"]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[('Gatwick', 'NNP'), ('airport', 'NN'), (',', ','), ('the', 'DT'), ('UK', 'NNP'), ('’', 'NNP'), ('s', 'VBD'), ('second', 'JJ'), ('largest', 'JJS'), ('airport', 'NN'), (',', ','), ('was', 'VBD'), ('closed', 'VBN'), ('for', 'IN'), ('most', 'JJS'), ('of', 'IN'), ('Wednesday', 'NNP'), ('night', 'NN'), ('and', 'CC'), ('Thursday', 'NNP'), ('morning', 'NN'), ('after', 'IN'), ('it', 'PRP'), ('received', 'VBD'), ('reports', 'NNS'), ('of', 'IN'), ('two', 'CD'), ('drones', 'NNS'), ('flying', 'VBG'), ('nearby', 'RB'), (',', ','), ('reports', 'VBZ'), ('The', 'DT'), ('Guardian', 'NNP'), ('.', '.')]\n",
      "[('The', 'DT'), ('airport', 'NN'), ('initially', 'RB'), ('had', 'VBD'), ('its', 'PRP$'), ('flights', 'NNS'), ('suspended', 'VBN'), ('at', 'IN'), ('9PM', 'CD'), ('on', 'IN'), ('Wednesday', 'NNP'), ('evening', 'NN'), ('after', 'IN'), ('the', 'DT'), ('drones', 'NNS'), ('were', 'VBD'), ('spotted', 'VBN'), (',', ','), ('and', 'CC'), ('although', 'IN'), ('it', 'PRP'), ('briefly', 'VBD'), ('reopened', 'VBN'), ('at', 'IN'), ('3am', 'CD'), (',', ','), ('it', 'PRP'), ('was', 'VBD'), ('forced', 'VBN'), ('to', 'TO'), ('close', 'VB'), ('once', 'RB'), ('more', 'JJR'), ('45', 'CD'), ('minutes', 'NNS'), ('later', 'RB'), ('after', 'IN'), ('the', 'DT'), ('drone', 'NN'), ('flights', 'NNS'), ('resumed', 'VBD'), ('.', '.')]\n",
      "[('As', 'IN'), ('of', 'IN'), ('9:15AM', 'CD'), (',', ','), ('Thursday', 'NNP'), ('morning', 'NN'), (',', ','), ('flights', 'NNS'), ('to', 'TO'), ('and', 'CC'), ('from', 'IN'), ('the', 'DT'), ('airport', 'NN'), ('remained', 'VBD'), ('suspended', 'JJ'), ('.', '.')]\n",
      "[('As', 'RB'), ('well', 'RB'), ('as', 'IN'), ('preventing', 'VBG'), ('any', 'DT'), ('flights', 'NNS'), ('from', 'IN'), ('taking', 'VBG'), ('off', 'RP'), (',', ','), ('the', 'DT'), ('suspension', 'NN'), ('means', 'VBZ'), ('that', 'IN'), ('numerous', 'JJ'), ('inbound', 'JJ'), ('flights', 'NNS'), ('had', 'VBD'), ('to', 'TO'), ('be', 'VB'), ('diverted', 'VBN'), ('to', 'TO'), ('other', 'JJ'), ('London-area', 'JJ'), ('airports', 'NNS'), ('including', 'VBG'), ('Luton', 'NNP'), (',', ','), ('Heathrow', 'NNP'), ('and', 'CC'), ('Stansted', 'NNP'), (',', ','), ('while', 'IN'), ('other', 'JJ'), ('flights', 'NNS'), ('were', 'VBD'), ('forced', 'VBN'), ('to', 'TO'), ('land', 'VB'), ('in', 'IN'), ('Paris', 'NNP'), ('and', 'CC'), ('Amsterdam', 'NNP'), ('.', '.')]\n",
      "[('In', 'IN'), ('total', 'JJ'), (',', ','), ('760', 'CD'), ('flights', 'NNS'), ('containing', 'VBG'), ('110,000', 'CD'), ('passengers', 'NNS'), ('were', 'VBD'), ('due', 'JJ'), ('to', 'TO'), ('either', 'DT'), ('take', 'VB'), ('off', 'RP'), ('or', 'CC'), ('land', 'NN'), ('from', 'IN'), ('Gatwick', 'NNP'), ('over', 'IN'), ('the', 'DT'), ('course', 'NN'), ('of', 'IN'), ('Thursday', 'NNP'), ('.', '.')]\n"
     ]
    }
   ],
   "source": [
    "for text in texts:\n",
    "    sentences = nltk.sent_tokenize(text)\n",
    "    for sentence in sentences:\n",
    "        word = nltk.word_tokenize(sentence)\n",
    "        tagged = nltk.pos_tag(word)\n",
    "        print(tagged)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
